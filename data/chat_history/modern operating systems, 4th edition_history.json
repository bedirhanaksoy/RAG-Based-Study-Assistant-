[
  {
    "role": "user",
    "content": "what is an operating system?"
  },
  {
    "role": "assistant",
    "content": "An operating system is the software that runs in kernel mode and provides a user with a better, simpler, cleaner model of the computer and handles managing all the resources just mentioned. It’s a complex system that sits between the hardware and the application programs, allowing applications to run effectively. It’s the software that turns the hardware into a usable interface for users.\n",
    "context": [
      {
        "page_number": 34,
        "sentence_chunk": "SEC.1.1 WHAT IS AN OPERATING SYSTEM?3 system (such as the file system) run in user space. In such systems, it is difficult to draw a clear boundary. Everything running in kernel mode is clearly part of the operating system, but some programs running outside it are arguably also part of it, or at least closely associated with it. Operating systems differ from user (i.e., application) programs in ways other than where they reside. In particular, they are huge, complex, and long-lived. The source code of the heart of an operating system like Linux or Windows is on the order of fiv e million lines of code or more. To conceive of what this means, think of printing out fiv e million lines in book form, with 50 lines per page and 1000 pages per volume (larger than this book). It would take 100 volumes to list an op- erating system of this size—essentially an entire bookcase."
      },
      {
        "page_number": 32,
        "sentence_chunk": "1 INTRODUCTION A modern computer consists of one or more processors, some main memory, disks, printers, a keyboard, a mouse, a display, network interfaces, and various other input/output devices. All in all, a complex system.oo If every application pro- grammer had to understand how all these things work in detail, no code would ever get written. Furthermore, managing all these components and using them optimally is an exceedingly challenging job. For this reason, computers are equipped with a layer of software called the operating system, whose job is to provide user pro- grams with a better, simpler, cleaner, model of the computer and to handle manag- ing all the resources just mentioned. Operating systems are the subject of this book. Most readers will have had some experience with an operating system such as Windows, Linux, FreeBSD, or OS X, but appearances can be deceiving. The pro- gram that users interact with, usually called the shell when it is text based and the GUI (Graphical User Interface)—which is pronounced ‘‘gooey’’—when it uses icons, is actually not part of the operating system, although it uses the operating system to get its work done. A simple overview of the main components under discussion here is given in Fig.1-1. Here we see the hardware at the bottom."
      },
      {
        "page_number": 34,
        "sentence_chunk": "11. Besides Windows, the other main example we will use throughout this book is UNIX and its variants and clones. It, too, has evolved over the years, with versions like System V, Solaris, and FreeBSD being derived from the original system, whereas Linux is a fresh code base, although very closely modeled on UNIX and highly compatible with it. We will use examples from UNIX throughout this book and look at Linux in detail in Chap.10. In this chapter we will briefly touch on a number of key aspects of operating systems, including what they are, their history, what kinds are around, some of the basic concepts, and their structure. We will come back to many of these important topics in later chapters in more detail.1.1 WHAT IS AN OPERATING SYSTEM?It is hard to pin down what an operating system is other than saying it is the software that runs in kernel mode—and even that is not always true. Part of the problem is that operating systems perform two essentially unrelated functions:"
      },
      {
        "page_number": 36,
        "sentence_chunk": "SEC.1.1 WHAT IS AN OPERATING SYSTEM?5 Operating system Hardware Ugly interface Beautiful interface Application programs Figure 1-2. Operating systems turn ugly hardware into beautiful abstractions. It should be noted that the operating system’s real customers are the applica- tion programs (via the application programmers, of course). They are the ones who deal directly with the operating system and its abstractions. In contrast, end users deal with the abstractions provided by the user interface, either a com- mand-line shell or a graphical interface. While the abstractions at the user interface may be similar to the ones provided by the operating system, this is not always the case. To make this point clearer, consider the normal Windows desktop and the line-oriented command prompt. Both are programs running on the Windows oper- ating system and use the abstractions Windows provides, but they offer very dif- ferent user interfaces."
      },
      {
        "page_number": 111,
        "sentence_chunk": "These parts are connected by buses. The basic concepts on which all operating systems are built are processes, memory management, I/O management, the file system, and security. Each of these will be treated in a subsequent chapter."
      }
    ]
  },
  {
    "role": "user",
    "content": "What are the responsibilities of the buses?"
  },
  {
    "role": "assistant",
    "content": "The buses in this system are designed to be versatile, capable of transferring data at various rates and serving different functions. They are categorized into several types, including PCIe, which is a high-speed bus used for peripherals and data transfer. The operating system must manage these buses to ensure efficient and coordinated operation of the system.",
    "context": [
      {
        "page_number": 63,
        "sentence_chunk": "This system has many buses (e.g., cache, memory, PCIe, PCI, USB, SATA, and DMI), each with a different transfer rate and function. The operating system must be aware of all of them for configuration and management. The main bus is the PCIe (Peripheral Component Interconnect Express) bus. The PCIe bus was invented by Intel as a successor to the older PCI bus, which in turn was a replacement for the original ISA (Industry Standard Architecture) bus. Capable of transferring tens of gigabits per second, PCIe is much faster than its predecessors. It is also very different in nature. Up to its creation in 2004, most buses were parallel and shared. A shared bus architecture means that multiple de- vices use the same wires to transfer data. Thus, when multiple devices have data to send, you need an arbiter to determine who can use the bus. In contrast, PCIe makes use of dedicated, point-to-point connections."
      },
      {
        "page_number": 763,
        "sentence_chunk": "To the right in Fig.10-3 are the other two key components of the Linux kernel. These are responsible for the memory and process management tasks. Memo- ry-management tasks include maintaining the virtual to physical-memory map- pings, maintaining a cache of recently accessed pages and implementing a good page-replacement policy, and on-demand bringing in new pages of needed code and data into memory. The key responsibility of the process-management component is the creation and termination of processes. It also includes the process scheduler, which chooses which process or, rather, thread to run next. As we shall see in the next section, the Linux kernel treats both processes and threads simply as executable entities, and will schedule them based on a global scheduling policy. Finally, code for signal handling also belongs to this component."
      },
      {
        "page_number": 580,
        "sentence_chunk": "This design means that the main CPU can offload some work to the network board, such as handling reliable trans- mission (if the underlying hardware can lose packets), multicasting (sending a packet to more than one destination), compression/decompression, encryption/de- cryption, and taking care of protection in a system that has multiple processes."
      },
      {
        "page_number": 391,
        "sentence_chunk": "For example, the driver must initialize the device, if needed. It may also need to manage its power requirements and log events. Many device drivers have a similar general structure. A typical driver starts out by checking the input parameters to see if they are valid. If not, an error is re- turned. If they are valid, a translation from abstract to concrete terms may be need- ed. For a disk driver, this may mean converting a linear block number into the head, track, sector, and cylinder numbers for the disk’s geometry. Next the driver may check if the device is currently in use. If it is, the request will be queued for later processing. If the device is idle, the hardware status will be examined to see if the request can be handled now."
      },
      {
        "page_number": 762,
        "sentence_chunk": "Process dispatching also happens when the kernel completes some operations and it is time to start up a user process again. The dispatching code is in assembler and is quite distinct from scheduling. Next, we divide the various kernel subsystems into three main components. The I/O component in Fig.10-3 contains all kernel pieces responsible for interact- ing with devices and performing network and storage I/O operations. At the high- est level, the I/O operations are all integrated under a VFS (Virtual File System) layer. That is, at the top level, performing a read operation on a file, whether it is in"
      }
    ]
  },
  {
    "role": "user",
    "content": "What are the benefits of using L1 cache inside of the processor?"
  },
  {
    "role": "assistant",
    "content": "The L1 cache inside the processor provides several benefits:\n\n*   **Fast Access:** It allows for significantly faster access to frequently used data and instructions compared to main memory, reducing latency.\n*   **Reduced Memory Latency:** By storing frequently accessed data closer to the processor, L1 cache minimizes the time it takes to retrieve data, improving overall system performance.\n*   **Improved Execution Speed:** Faster data access translates directly into quicker execution of instructions and programs.\n",
    "context": [
      {
        "page_number": 57,
        "sentence_chunk": "The L1 caches are typically 16 KB each. In addition, there is often a second cache, called the L2 cache, that holds several megabytes of recently used memory words. The difference between the L1 and L2 caches lies in the timing. Access to the L1 cache is done without any delay, whereas access to the L2 cache involves a delay of one or two clock cycles. On multicore chips, the designers have to decide where to place the caches. In Fig.1-8(a), a single L2 cache is shared by all the cores. This approach is used in Intel multicore chips. In contrast, in Fig.1-8(b), each core has its own L2 cache."
      },
      {
        "page_number": 56,
        "sentence_chunk": "Similarly, the results of converting long path names like /home/ast/projects/minix3/src/kernel/clock.c into the disk address where the file is located can be cached to avoid repeated lookups. Finally, when the address of a Web page (URL) is converted to a network address (IP address), the result can be cached for future use. Many other uses exist. In any caching system, several questions come up fairly soon, including: 1. When to put a new item into the cache.2. Which cache line to put the new item in.3. Which item to remove from the cache when a slot is needed.4."
      },
      {
        "page_number": 57,
        "sentence_chunk": "26 INTRODUCTION CHAP.1 Not every question is relevant to every caching situation. For caching lines of main memory in the CPU cache, a new item will generally be entered on every cache miss. The cache line to use is generally computed by using some of the high-order bits of the memory address referenced. For example, with 4096 cache lines of 64 bytes and 32 bit addresses, bits 6 through 17 might be used to specify the cache line, with bits 0 to 5 the byte within the cache line. In this case, the item to remove is the same one as the new data goes into, but in other systems it might not be. Finally, when a cache line is rewritten to main memory (if it has been modified since it was cached), the place in memory to rewrite it to is uniquely determined by the address in question. Caches are such a good idea that modern CPUs have two of them. The first level or L1 cache is always inside the CPU and usually feeds decoded instructions into the CPU’s execution engine. Most chips have a second L1 cache for very heavily used data words."
      },
      {
        "page_number": 559,
        "sentence_chunk": "The shared-cache design of Fig.1-8(a), which Intel uses in many of its processors, does not occur in other multiprocessors. A shared L2 or L3 cache can affect performance. If one core needs a lot of cache memory and the others do not, this design allows the cache hog to take whatever it needs. On the other hand, the shared cache also makes it possible for a greedy core to hurt the other cores. An area in which CMPs differ from their larger cousins is fault tolerance. Be- cause the CPUs are so closely connected, failures in shared components may bring down multiple CPUs at once, something unlikely in traditional multiprocessors. In addition to symmetric multicore chips, where all the cores are identical, an- other common category of multicore chip is the System On a Chip (SoC). These chips have one or more main CPUs, but also special-purpose cores, such as video and audio decoders, cryptoprocessors, network interfaces, and more, leading to a complete computer system on a chip. Manycore Chips Multicore simply means ‘‘more than one core,’’ but when the number of cores grows well beyond the reach of finger counting, we use another name."
      },
      {
        "page_number": 558,
        "sentence_chunk": "Each of the 12 physical cores has 32 KB of L1 instruction cache and 32 KB of L1 data cache. Each one also has 256 KB of L2 cache. Finally, the 12 cores share 30 MB of L3 cache. While the CPUs may or may not share caches (see, for example, Fig.1-8), they always share main memory, and this memory is consistent in the sense that there is always a unique value for each memory word. Special hardware circuitry makes"
      }
    ]
  },
  {
    "role": "user",
    "content": "Generate flashcards about: Multithreading challenges"
  },
  {
    "role": "flashcard",
    "content": [
      {
        "question": "What is a potential consequence of scheduling two threads on the same CPU?",
        "answer": "When two threads are scheduled on the same CPU, the kernel may choose to run them simultaneously, leading to a situation where the CPU is idle while both threads are running. This can result in a loss of efficiency as the CPU is not utilized to its full potential.",
        "context": [
          {
            "page_number": 55,
            "sentence_chunk": "24 INTRODUCTION CHAP.1 time, it may inadvertently schedule two threads on the same CPU, with the other CPU completely idle. This choice is far less efficient than using one thread on each CPU. Beyond multithreading, many CPU chips now hav e four, eight, or more com- plete processors or cores on them. The multicore chips of Fig.1-8 effectively carry four minichips on them, each with its own independent CPU. (The caches will be explained below.)Some processors, like Intel Xeon Phi and the Tilera TilePro, al- ready sport more than 60 cores on a single chip. Making use of such a multicore chip will definitely require a multiprocessor operating system. Incidentally, in terms of sheer numbers, nothing beats a modern GPU (Graph- ics Processing Unit)."
          },
          {
            "page_number": 573,
            "sentence_chunk": "For example, if the threads of a process communicate a lot, it is useful to have them running at the same time. Scheduling multiple threads at the same time across multiple CPUs is called space sharing. The simplest space-sharing algorithm works like this. Assume that an entire group of related threads is created at once. At the time it is created, the scheduler checks to see if there are as many free CPUs as there are threads. If there are, each thread is given its own dedicated (i.e., nonmultiprogrammed) CPU and they all start. If there are not enough CPUs, none of the threads are started until enough CPUs are available. Each thread holds onto its CPU until it terminates, at which time the CPU is put back into the pool of available CPUs. If a thread blocks on I/O, it continues to hold the CPU, which is simply idle until the thread wakes up. When the next batch of threads appears, the same algorithm is applied."
          },
          {
            "page_number": 180,
            "sentence_chunk": "If only one CPU is available, a choice has to be made which process to run next. The part of the oper- ating system that makes the choice is called the scheduler, and the algorithm it uses is called the scheduling algorithm. These topics form the subject matter of the following sections. Many of the same issues that apply to process scheduling also apply to thread scheduling, although some are different. When the kernel manages threads, sched- uling is usually done per thread, with little or no regard to which process the thread belongs. Initially we will focus on scheduling issues that apply to both processes and threads. Later on we will explicitly look at thread scheduling and some of the unique issues it raises. We will deal with multicore chips in Chap.8."
          },
          {
            "page_number": 197,
            "sentence_chunk": "However, its antisocial be- havior will not affect other processes. They will get whatever the scheduler con- siders their appropriate share, no matter what is going on inside process A. Now consider the case that A’s threads have relatively little work to do per CPU burst, for example, 5 msec of work within a 50-msec quantum. Consequently, each one runs for a little while, then yields the CPU back to the thread scheduler. This might lead to the sequence A1, A2, A3, A1, A2, A3, A1, A2, A3, A1, before the kernel switches to process B. This situation is illustrated in Fig.2-44(a). Process A Process B Process B Process A 1. Kernel picks a process 1. Kernel picks a thread Possible:     A1, A2, A3, A1, A2, A3 Also possible: A1, B1, A2, B2, A3, B3 Possible:     A1, A2, A3, A1, A2, A3 Not possible:  A1, B1, A2, B2, A3, B3 (a) (b) Order in which threads run 2. Run-time   system   picks a   thread 1 2 3 1 3 2 Figure 2-44. (a) Possible scheduling of user-level threads with a 50-msec proc- ess quantum and threads that run 5 msec per CPU burst. ("
          },
          {
            "page_number": 573,
            "sentence_chunk": "542 MULTIPLE PROCESSOR SYSTEMS CHAP.8 keep a thread on the same CPU for its entire lifetime, cache affinity is maximized. However, if a CPU has no threads to run, it takes one from another CPU rather than go idle. Tw o-level scheduling has three benefits. First, it distributes the load roughly ev enly over the available CPUs. Second, advantage is taken of cache affinity where possible. Third, by giving each CPU its own ready list, contention for the ready lists is minimized because attempts to use another CPU’s ready list are rel- atively infrequent. Space Sharing The other general approach to multiprocessor scheduling can be used when threads are related to one another in some way. Earlier we mentioned the example of parallel make as one case. It also often occurs that a single process has multiple threads that work together."
          }
        ]
      },
      {
        "question": "What is the challenge in achieving multithreaded behavior?",
        "answer": "The challenge in achieving multithreaded behavior is maintaining the correct traditional UNIX semantics. Specifically, libraries like `fork` present a problem: if a process with multiple threads performs a fork system call, all other threads must be created in the new process, even if one thread is blocked. If a thread is blocked, the corresponding thread in the new process must also be blocked, and the thread that was waiting for the keyboard input is forced to wait. Furthermore, library procedures like `malloc` can have inconsistent memory management, with tables updating during busy periods, potentially leading to invalid pointer usage when a thread switch occurs.",
        "context": [
          {
            "page_number": 55,
            "sentence_chunk": "24 INTRODUCTION CHAP.1 time, it may inadvertently schedule two threads on the same CPU, with the other CPU completely idle. This choice is far less efficient than using one thread on each CPU. Beyond multithreading, many CPU chips now hav e four, eight, or more com- plete processors or cores on them. The multicore chips of Fig.1-8 effectively carry four minichips on them, each with its own independent CPU. (The caches will be explained below.)Some processors, like Intel Xeon Phi and the Tilera TilePro, al- ready sport more than 60 cores on a single chip. Making use of such a multicore chip will definitely require a multiprocessor operating system. Incidentally, in terms of sheer numbers, nothing beats a modern GPU (Graph- ics Processing Unit)."
          },
          {
            "page_number": 134,
            "sentence_chunk": "1, some CPUs have direct hardware support for multithreading and allow thread switches to happen on a nanosecond time scale. In Fig.2-11(a) we see three traditional processes. Each process has its own ad- dress space and a single thread of control. In contrast, in Fig.2-11(b) we see a sin- gle process with three threads of control. Although in both cases we have three threads, in Fig.2-11(a) each of them operates in a different address space, whereas in Fig.2-11(b) all three of them share the same address space. When a multithreaded process is run on a single-CPU system, the threads take turns running."
          },
          {
            "page_number": 134,
            "sentence_chunk": "In Fig.2-1, we saw how multiprogramming of processes works. By switching back and forth among multiple processes, the system gives the illusion of separate sequential processes running in parallel. Multithreading works the same way. The CPU switches rapidly back and forth among the threads, providing the illusion that the threads are running in parallel, albeit on a slower CPU than the real one. With three compute-bound threads in a process, the threads would appear to be running in parallel, each one on a CPU with one-third the speed of the real CPU. Different threads in a process are not as independent as different processes. All threads have exactly the same address space, which means that they also share the"
          },
          {
            "page_number": 774,
            "sentence_chunk": "In order to better understand the unique capabilities pro- vided by the Linux model, we start with a discussion of some of the challenging decisions present in multithreaded systems. The main issue in introducing threads is maintaining the correct traditional UNIX semantics. First consider fork. Suppose that a process with multiple (kernel) threads does a fork system call. Should all the other threads be created in the new process?For the moment, let us answer that question with yes. Suppose that one of the other threads was blocked reading from the keyboard. Should the corres- ponding thread in the new process also be blocked reading from the keyboard?If so, which one gets the next line typed?If not, what should that thread be doing in the new process?"
          },
          {
            "page_number": 149,
            "sentence_chunk": "118 PROCESSES AND THREADS CHAP.2 The next problem in turning a single-threaded program into a multithreaded one is that many library procedures are not reentrant. That is, they were not de- signed to have a second call made to any giv en procedure while a previous call has not yet finished. For example, sending a message over the network may well be programmed to assemble the message in a fixed buffer within the library, then to trap to the kernel to send it. What happens if one thread has assembled its message in the buffer, then a clock interrupt forces a switch to a second thread that im- mediately overwrites the buffer with its own message?Similarly, memory-allocation procedures such as malloc in UNIX, maintain crucial tables about memory usage, for example, a linked list of available chunks of memory. While malloc is busy updating these lists, they may temporarily be in an inconsistent state, with pointers that point nowhere. If a thread switch occurs while the tables are inconsistent and a new call comes in from a different thread, an invalid pointer may be used, leading to a program crash. Fixing all these problems effectively means rewriting the entire library. Doing so is a nontrivial activity with a real possibility of introducing subtle errors."
          }
        ]
      },
      {
        "question": "How does the concurrent execution of threads differ from the concurrent execution of processes?",
        "answer": "The concurrent execution of threads differs from the concurrent execution of processes in that threads share the same address space, while processes maintain separate address spaces. This means threads can interfere with each other's data, while processes are isolated from one another. In essence, threads are like multiple streams of execution running concurrently within a single program, whereas processes are distinct and independent execution units.",
        "context": [
          {
            "page_number": 134,
            "sentence_chunk": "SEC.2.2 THREADS 103 separate them; this is where threads come in. First we will look at the classical thread model; after that we will examine the Linux thread model, which blurs the line between processes and threads. One way of looking at a process is that it is a way to group related resources together. A process has an address space containing program text and data, as well as other resources. These resources may include open files, child processes, pend- ing alarms, signal handlers, accounting information, and more. By putting them together in the form of a process, they can be managed more easily. The other concept a process has is a thread of execution, usually shortened to just thread. The thread has a program counter that keeps track of which instruc- tion to execute next. It has registers, which hold its current working variables."
          },
          {
            "page_number": 134,
            "sentence_chunk": "It has a stack, which contains the execution history, with one frame for each proce- dure called but not yet returned from. Although a thread must execute in some process, the thread and its process are different concepts and can be treated sepa- rately. Processes are used to group resources together; threads are the entities scheduled for execution on the CPU. What threads add to the process model is to allow multiple executions to take place in the same process environment, to a large degree independent of one anoth- er. Having multiple threads running in parallel in one process is analogous to hav- ing multiple processes running in parallel in one computer. In the former case, the threads share an address space and other resources. In the latter case, processes share physical memory, disks, printers, and other resources. Because threads have some of the properties of processes, they are sometimes called lightweight pro- cesses. The term multithreading is also used to describe the situation of allowing multiple threads in the same process. As we saw in Chap."
          },
          {
            "page_number": 134,
            "sentence_chunk": "In Fig.2-1, we saw how multiprogramming of processes works. By switching back and forth among multiple processes, the system gives the illusion of separate sequential processes running in parallel. Multithreading works the same way. The CPU switches rapidly back and forth among the threads, providing the illusion that the threads are running in parallel, albeit on a slower CPU than the real one. With three compute-bound threads in a process, the threads would appear to be running in parallel, each one on a CPU with one-third the speed of the real CPU. Different threads in a process are not as independent as different processes. All threads have exactly the same address space, which means that they also share the"
          },
          {
            "page_number": 116,
            "sentence_chunk": "2 PROCESSES AND THREADS We are now about to embark on a detailed study of how operating systems are designed and constructed. The most central concept in any operating system is the process: an abstraction of a running program. Everything else hinges on this con- cept, and the operating system designer (and student) should have a thorough un- derstanding of what a process is as early as possible. Processes are one of the oldest and most important abstractions that operating systems provide. They support the ability to have (pseudo) concurrent operation ev en when there is only one CPU available. They turn a single CPU into multiple virtual CPUs. Without the process abstraction, modern computing could not exist. In this chapter we will go into considerable detail about processes and their first cousins, threads.2.1 PROCESSES All modern computers often do several things at the same time. People used to working with computers may not be fully aware of this fact, so a few examples may make the point clearer."
          },
          {
            "page_number": 118,
            "sentence_chunk": "So when we say that a CPU can really run only one process at a time, if there are two cores (or CPUs) each of them can run only one process at a time. With the CPU switching back and forth among the processes, the rate at which a process performs its computation will not be uniform and probably not even reproducible if the same processes are run again. Thus, processes must not be pro- grammed with built-in assumptions about timing. Consider, for example, an audio process that plays music to accompany a high-quality video run by another device. Because the audio should start a little later than the video, it signals the video ser- ver to start playing, and then runs an idle loop 10,000 times before playing back the audio. All goes well, if the loop is a reliable timer, but if the CPU decides to switch to another process during the idle loop, the audio process may not run again until the corresponding video frames have already come and gone, and the video and audio will be annoyingly out of sync. When a process has critical real-time re- quirements like this, that is, particular events must occur within a specified number of milliseconds, special measures must be taken to ensure that they do occur. Nor- mally, howev er, most processes are not affected by the underlying multiprogram- ming of the CPU or the relative speeds of different processes. The difference between a process and a program is subtle, but absolutely cru- cial. An analogy may help you here."
          }
        ]
      }
    ]
  },
  {
    "role": "user",
    "content": "What is multiprogramming"
  },
  {
    "role": "assistant",
    "content": "Multiprogramming is a technique where multiple processes are allowed to share a single CPU. The CPU rapidly switches between these processes, giving the illusion of parallel execution.\n",
    "context": [
      {
        "page_number": 134,
        "sentence_chunk": "In Fig.2-1, we saw how multiprogramming of processes works. By switching back and forth among multiple processes, the system gives the illusion of separate sequential processes running in parallel. Multithreading works the same way. The CPU switches rapidly back and forth among the threads, providing the illusion that the threads are running in parallel, albeit on a slower CPU than the real one. With three compute-bound threads in a process, the threads would appear to be running in parallel, each one on a CPU with one-third the speed of the real CPU. Different threads in a process are not as independent as different processes. All threads have exactly the same address space, which means that they also share the"
      },
      {
        "page_number": 117,
        "sentence_chunk": "All this activity has to be managed, and a multiprogramming system supporting multiple processes comes in very handy here. In any multiprogramming system, the CPU switches from process to process quickly, running each for tens or hundreds of milliseconds. While, strictly speak- ing, at any one instant the CPU is running only one process, in the course of 1 sec- ond it may work on several of them, giving the illusion of parallelism. Sometimes people speak of pseudoparallelism in this context, to contrast it with the true hard- ware parallelism of multiprocessor systems (which have two or more CPUs shar- ing the same physical memory). Keeping track of multiple, parallel activities is hard for people to do. Therefore, operating system designers over the years have ev olved a conceptual model (sequential processes) that makes parallelism easier to deal with. That model, its uses, and some of its consequences form the subject of this chapter.2.1.1 The Process Model In this model, all the runnable software on the computer, sometimes including the operating system, is organized into a number of sequential processes, or just processes for short. A process is just an instance of an executing program, includ- ing the current values of the program counter, registers, and variables. Con- ceptually, each process has its own virtual CPU."
      },
      {
        "page_number": 551,
        "sentence_chunk": "520 MULTIPLE PROCESSOR SYSTEMS CHAP.8 introduction to the relevant hardware. Then we move on to the software, especially the operating system issues for that type of system. As we will see, in each case different issues are present and different approaches are needed.8.1 MULTIPROCESSORS A shared-memory multiprocessor (or just multiprocessor henceforth) is a computer system in which two or more CPUs share full access to a common RAM. A program running on any of the CPUs sees a normal (usually paged) virtual ad- dress space. The only unusual property this system has is that the CPU can write some value into a memory word and then read the word back and get a different value (because another CPU has changed it). When organized correctly, this prop- erty forms the basis of interprocessor communication: one CPU writes some data into memory and another one reads the data out. For the most part, multiprocessor operating systems are normal operating sys- tems. They handle system calls, do memory management, provide a file system, and manage I/O devices."
      },
      {
        "page_number": 1079,
        "sentence_chunk": "1048 READING LIST AND BIBLIOGRAPHY CHAP.13 DIJKSTRA, E. W.: ‘‘Co-operating Sequential Processes,’’ in Programming Languages, Genuys, F. (Ed.),London: Academic Press, 1965. DIJKSTRA, E. W.: ‘‘The Structure of THE Multiprogramming System,’’ Commun.of the ACM, vol.11, pp.341–346, May 1968. DUBOIS, M., SCHEURICH, C., and BRIGGS, F. A.: ‘‘Synchronization, Coherence, and Event Ordering in Multiprocessors,’’ Computer, vol.21, pp.9–21, Feb. 1988."
      },
      {
        "page_number": 118,
        "sentence_chunk": "SEC.2.1 PROCESSES 87 a long enough time interval, all the processes have made progress, but at any giv en instant only one process is actually running. A B C D D C B A Process switch One program counter Four program counters Process Time B C D A (a) (b) (c) Figure 2-1. (a) Multiprogramming four programs. (b) Conceptual model of four independent, sequential processes. (c) Only one program is active at once. In this chapter, we will assume there is only one CPU. Increasingly, howev er, that assumption is not true, since new chips are often multicore, with two, four, or more cores. We will look at multicore chips and multiprocessors in general in Chap.8, but for the time being, it is simpler just to think of one CPU at a time."
      }
    ]
  }
]